{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48b0b479",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3: Web APIs & NLP\n",
    "\n",
    "Part 1 of 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9f001c",
   "metadata": {},
   "source": [
    "**Problem Statement**\n",
    "\n",
    "On a typical workday while performing maintenance, an engineer accidentally deleted multiple posts from the subreddits r/nottheonion and r/theonion. The former writes true stories that are mind-blowingly ridiculous, so much so that one would have thought it's from the latter which publishes satirical articles on international, national and local news. Despite the engineer's quick follow-up action, it was unfortunate that he could only recover the titles of the lost posts. \n",
    "\n",
    "Our team has been engaged to build a classification model which would train on posts submitted before 01 Jan 2022 to correctly sort the recovered posts back to their respective subreddits, r/nottheonion and r/theonion, based solely on the post titles.\n",
    "\n",
    "In employing NLP to help us build various models such as the Multinomial Naive Bayes, Random Forest, Logistic Regression etc, we will peg success to the initial accuracy scores across our cross validations. Beyond that, we will dive deeper into studying the confusion matrices together with the f1 scores to pick out the best model(s).\n",
    "\n",
    "As it is now, our primary stakeholders - the volunteer moderators of each subreddit thread - have to spend a substantial amount of time reviewing user reports and deleting spam posts from the subreddits. This is especially so with the increase in bots spamming subreddits with irrelevant posts. With this classifier model, we can use it as a proof of concept for the development of an automated moderator which would automatically delete posts that do not belong to the subreddit that they have been posted to. This in turn helps to free up time for our human moderators. As a consequence, our secondary stakeholders - the subreddit community as a whole - can enjoy their daily reads without being bothered by disparate frills.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72a1da3",
   "metadata": {},
   "source": [
    "**Data Collection**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24e0a49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psaw in c:\\users\\krispy\\anaconda3\\lib\\site-packages (0.1.0)\n",
      "Requirement already satisfied: Click in c:\\users\\krispy\\anaconda3\\lib\\site-packages (from psaw) (7.1.2)\n",
      "Requirement already satisfied: requests in c:\\users\\krispy\\anaconda3\\lib\\site-packages (from psaw) (2.25.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\krispy\\anaconda3\\lib\\site-packages (from requests->psaw) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\krispy\\anaconda3\\lib\\site-packages (from requests->psaw) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\krispy\\anaconda3\\lib\\site-packages (from requests->psaw) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\krispy\\anaconda3\\lib\\site-packages (from requests->psaw) (2020.12.5)\n"
     ]
    }
   ],
   "source": [
    "# API scrape \n",
    "!pip install psaw\n",
    "from psaw import PushshiftAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5acdb5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef54deca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to input the subreddit threads subsequently.\n",
    "def scrape_data(subreddit):\n",
    "    \n",
    "    # Instantiate \n",
    "    api = PushshiftAPI()\n",
    "\n",
    "    # Create list of scraped data\n",
    "    scrape_list = list(api.search_submissions(subreddit=subreddit,\n",
    "                                before= 1640995200, #Specifying this so we scrape the same batch of posts.\n",
    "                                filter=['title', 'subreddit', 'num_comments', 'author', 'subreddit_subscribers', 'score', 'domain', 'created_utc'],\n",
    "                                limit=3000)) \n",
    "\n",
    "    #Filter list to only show Subreddit titles and Subreddit category \n",
    "    clean_scrape_lst = []\n",
    "    for i in range(len(scrape_list)):\n",
    "        scrape_dict = {}\n",
    "        scrape_dict['subreddit'] = scrape_list[i][5]\n",
    "        scrape_dict['author'] = scrape_list[i][0]\n",
    "        scrape_dict['domain'] = scrape_list[i][2]\n",
    "        scrape_dict['title'] = scrape_list[i][7]\n",
    "        scrape_dict['num_comments'] = scrape_list[i][3]\n",
    "        scrape_dict['score'] = scrape_list[i][4]\n",
    "        scrape_dict['timestamp'] = scrape_list[i][1]\n",
    "        clean_scrape_lst.append(scrape_dict)\n",
    "\n",
    "    # Show number of subscribers\n",
    "    print(subreddit, 'subscribers:',scrape_list[1][6])\n",
    "    \n",
    "    # Return list of scraped data\n",
    "    return clean_scrape_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cab277",
   "metadata": {},
   "source": [
    "I intend to collect 2000 unique posts from each subreddit so I scrap 3000 knowing that there might be repeat posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64cba31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Krispy\\anaconda3\\lib\\site-packages\\psaw\\PushshiftAPI.py:252: UserWarning: Not all PushShift shards are active. Query results may be incomplete\n",
      "  warnings.warn(shards_down_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theonion subscribers: 165298\n",
      "df_onion shape: (2996, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>mothershipq</td>\n",
       "      <td>theonion.com</td>\n",
       "      <td>Surgeon Kind Of Pissed Patient Seeing Her Defo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1640973300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>-ImYourHuckleberry-</td>\n",
       "      <td>theartnewspaper.com</td>\n",
       "      <td>McDonald’s blocked from building drive-through...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1640971771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>dwaxe</td>\n",
       "      <td>theonion.com</td>\n",
       "      <td>Gwyneth Paltrow Touts New Diamond-Encrusted Tr...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1640955671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>dwaxe</td>\n",
       "      <td>theonion.com</td>\n",
       "      <td>Artist Crafting Music Box Hopes It Delights At...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1640955669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>dwaxe</td>\n",
       "      <td>theonion.com</td>\n",
       "      <td>Homeowner Trying To Smoke Out Snakes Accidenta...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1640955668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit               author               domain  \\\n",
       "0  TheOnion          mothershipq         theonion.com   \n",
       "1  TheOnion  -ImYourHuckleberry-  theartnewspaper.com   \n",
       "2  TheOnion                dwaxe         theonion.com   \n",
       "3  TheOnion                dwaxe         theonion.com   \n",
       "4  TheOnion                dwaxe         theonion.com   \n",
       "\n",
       "                                               title  num_comments  score  \\\n",
       "0  Surgeon Kind Of Pissed Patient Seeing Her Defo...             0      1   \n",
       "1  McDonald’s blocked from building drive-through...             1      1   \n",
       "2  Gwyneth Paltrow Touts New Diamond-Encrusted Tr...             0      1   \n",
       "3  Artist Crafting Music Box Hopes It Delights At...             0      1   \n",
       "4  Homeowner Trying To Smoke Out Snakes Accidenta...             0      1   \n",
       "\n",
       "    timestamp  \n",
       "0  1640973300  \n",
       "1  1640971771  \n",
       "2  1640955671  \n",
       "3  1640955669  \n",
       "4  1640955668  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call function and create DataFrame\n",
    "df_onion = pd.DataFrame(scrape_data('theonion'))\n",
    "\n",
    "# Save data to csv\n",
    "df_onion.to_csv('../datasets/the_onion.csv')\n",
    "\n",
    "# Shape of DataFrame\n",
    "print(f'df_onion shape: {df_onion.shape}')\n",
    "\n",
    "# Show head\n",
    "df_onion.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "020e8084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Krispy\\anaconda3\\lib\\site-packages\\psaw\\PushshiftAPI.py:252: UserWarning: Not all PushShift shards are active. Query results may be incomplete\n",
      "  warnings.warn(shards_down_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nottheonion subscribers: 20438921\n",
      "df_not_onion shape: (2997, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nottheonion</td>\n",
       "      <td>Taco_duck68</td>\n",
       "      <td>wral.com</td>\n",
       "      <td>Man attempts to pay for car with rap, steals p...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1640995192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nottheonion</td>\n",
       "      <td>BlackNingaa</td>\n",
       "      <td>bloodyelbow.com</td>\n",
       "      <td>Former UFC fighter reveals past as sex worker ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1640994707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nottheonion</td>\n",
       "      <td>Lopsided_File_1642</td>\n",
       "      <td>facebook.com</td>\n",
       "      <td>Log into Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1640991506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nottheonion</td>\n",
       "      <td>SkinnyWhiteGirl19</td>\n",
       "      <td>theartnewspaper.com</td>\n",
       "      <td>McDonald’s blocked from building drive-through...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1640990429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nottheonion</td>\n",
       "      <td>kids-cake-and-crazy</td>\n",
       "      <td>kjrh.com</td>\n",
       "      <td>Legendary actress Betty White dies at 99 on Ne...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1640989181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     subreddit               author               domain  \\\n",
       "0  nottheonion          Taco_duck68             wral.com   \n",
       "1  nottheonion          BlackNingaa      bloodyelbow.com   \n",
       "2  nottheonion   Lopsided_File_1642         facebook.com   \n",
       "3  nottheonion    SkinnyWhiteGirl19  theartnewspaper.com   \n",
       "4  nottheonion  kids-cake-and-crazy             kjrh.com   \n",
       "\n",
       "                                               title  num_comments  score  \\\n",
       "0  Man attempts to pay for car with rap, steals p...             0      1   \n",
       "1  Former UFC fighter reveals past as sex worker ...             1      1   \n",
       "2                                  Log into Facebook             1      1   \n",
       "3  McDonald’s blocked from building drive-through...             0      1   \n",
       "4  Legendary actress Betty White dies at 99 on Ne...             0      1   \n",
       "\n",
       "    timestamp  \n",
       "0  1640995192  \n",
       "1  1640994707  \n",
       "2  1640991506  \n",
       "3  1640990429  \n",
       "4  1640989181  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call function and create DataFrame\n",
    "df_not_onion = pd.DataFrame(scrape_data('nottheonion'))\n",
    "\n",
    "# Save data to csv\n",
    "df_not_onion.to_csv('../datasets/not_onion.csv')\n",
    "\n",
    "# Shape of DataFrame\n",
    "print(f'df_not_onion shape: {df_not_onion.shape}')\n",
    "\n",
    "# Show head\n",
    "df_not_onion.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdb5e5c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2783"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the unique titles in r/TheOnion df.\n",
    "df_onion['title'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e0d4f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2338"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the unique titles in r/nottheonion df.\n",
    "df_not_onion['title'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee3bf31",
   "metadata": {},
   "source": [
    "As expected, there are repeated posts in both subreddit threads. To ensure that I take only useful rows that contribute to a successful model built, I'll take only unique titles further downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f36d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the duplicate titles.\n",
    "df_onion.drop_duplicates(subset=['title'], inplace=True)\n",
    "df_not_onion.drop_duplicates(subset=['title'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a7071d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2783, 7)\n",
      "(2338, 7)\n"
     ]
    }
   ],
   "source": [
    "print(df_onion.shape)\n",
    "print(df_not_onion.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "278faa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing so I take 2000 unique titles from each df, keeping to the same number from each df.\n",
    "df_onion = df_onion.iloc[0:2000]\n",
    "df_not_onion = df_not_onion.iloc[0:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a0236dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>mothershipq</td>\n",
       "      <td>theonion.com</td>\n",
       "      <td>Surgeon Kind Of Pissed Patient Seeing Her Defo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1640973300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>-ImYourHuckleberry-</td>\n",
       "      <td>theartnewspaper.com</td>\n",
       "      <td>McDonald’s blocked from building drive-through...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1640971771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>dwaxe</td>\n",
       "      <td>theonion.com</td>\n",
       "      <td>Gwyneth Paltrow Touts New Diamond-Encrusted Tr...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1640955671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>dwaxe</td>\n",
       "      <td>theonion.com</td>\n",
       "      <td>Artist Crafting Music Box Hopes It Delights At...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1640955669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>dwaxe</td>\n",
       "      <td>theonion.com</td>\n",
       "      <td>Homeowner Trying To Smoke Out Snakes Accidenta...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1640955668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit               author               domain  \\\n",
       "0  TheOnion          mothershipq         theonion.com   \n",
       "1  TheOnion  -ImYourHuckleberry-  theartnewspaper.com   \n",
       "2  TheOnion                dwaxe         theonion.com   \n",
       "3  TheOnion                dwaxe         theonion.com   \n",
       "4  TheOnion                dwaxe         theonion.com   \n",
       "\n",
       "                                               title  num_comments  score  \\\n",
       "0  Surgeon Kind Of Pissed Patient Seeing Her Defo...             0      1   \n",
       "1  McDonald’s blocked from building drive-through...             1      1   \n",
       "2  Gwyneth Paltrow Touts New Diamond-Encrusted Tr...             0      1   \n",
       "3  Artist Crafting Music Box Hopes It Delights At...             0      1   \n",
       "4  Homeowner Trying To Smoke Out Snakes Accidenta...             0      1   \n",
       "\n",
       "    timestamp  \n",
       "0  1640973300  \n",
       "1  1640971771  \n",
       "2  1640955671  \n",
       "3  1640955669  \n",
       "4  1640955668  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining all 4000 rows of data from both subreddit threads together using concatenation. \n",
    "combined_df = pd.concat([df_onion, df_not_onion],axis=0)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3458d905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 7)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the shape.\n",
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a410591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To export the data.\n",
    "combined_df.to_csv('../datasets/combined_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20438c49",
   "metadata": {},
   "source": [
    "With the df from both subreddits combined, the next codebook will be where I clean up the data and carry out modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d305c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
